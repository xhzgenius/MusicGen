{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\NLP2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, MusicgenForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "\n",
    "model_name = \"facebook/musicgen-medium\"  # 可选：small, medium, large\n",
    "# 初次使用记得去掉local_files_only=True\n",
    "processor = AutoProcessor.from_pretrained(model_name, local_files_only=True)\n",
    "model = MusicgenForConditionalGeneration.from_pretrained(model_name, local_files_only=True).half().to(device)\n",
    "# model.half()解决精度问题报错"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from datasets import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def process_data(batch):\n",
    "    # 加载音频并标准化\n",
    "    audio, sr = librosa.load(os.path.join(\"./data/datashare/\", batch[\"location\"]), sr=32000)\n",
    "    current_length = len(audio)\n",
    "    target_length = 320000 # 10s*32000/s\n",
    "    if current_length >= target_length:\n",
    "        padded_audio = audio[:target_length]  # 如果音频过长，截断\n",
    "    else:\n",
    "        # 计算需要补零的数量\n",
    "        padding_length = target_length - current_length\n",
    "        # 在末尾补零（静音）\n",
    "        padded_audio = np.pad(audio, (0, padding_length), mode='constant').flatten()\n",
    "    \n",
    "    # 使用 processor 处理文本和音频\n",
    "    inputs = processor(\n",
    "        text=[batch[\"main_caption\"]],\n",
    "        audio=[padded_audio],\n",
    "        sampling_rate=32000,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "    inputs[\"input_ids\"] = inputs[\"input_ids\"].flatten()\n",
    "    # inputs[\"input_values\"] = inputs[\"input_values\"].flatten()\n",
    "    # print(inputs[\"input_ids\"].shape, inputs[\"input_values\"].shape)\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"amaai-lab/MusicBench\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 800/800 [00:09<00:00, 84.63 examples/s] \n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(process_data, batched=False)\n",
    "# dataset = dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "dataset = dataset.train_test_split(0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置lora参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,718,592 || all params: 2,009,969,730 || trainable%: 0.2348\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# 定义 LoRA 配置\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                  # LoRA 的秩（Rank）\n",
    "    lora_alpha=32,        # 缩放因子\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # 目标模块（MusicGen 的注意力层）\n",
    "    lora_dropout=0.05,    # Dropout 率\n",
    "    bias=\"none\",          # 不调整偏置\n",
    "    task_type=\"CAUSAL_LM\", # 因果语言模型任务\n",
    ")\n",
    "\n",
    "# 应用 LoRA\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "lora_model.print_trainable_parameters()  # 查看可训练参数（应远小于原始模型）\n",
    "lora_model.save_pretrained(\"./outputs/musicgen-lora/initial_lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./outputs/musicgen-lora\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=1e-4,  # LoRA 需要更高的学习率\n",
    "    fp16=True,           # 混合精度训练\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'MusicBench',\n",
       " 'location': 'data/tmabzx6yxqs.wav',\n",
       " 'main_caption': 'This kids song features groovy synth keys, chords, simple piano melody, echoing and wide pig oink sound effects, shimmering shakers, muffled claps, soft kick hits and groovy bass. It sounds fun and happy - like something that children would listen to in their favorite tv shows.',\n",
       " 'alt_caption': 'This kids song features groovy synth keys, chords, simple piano melody, echoing and wide pig oink sound effects, shimmering shakers, muffled claps, soft kick hits and groovy bass. It sounds fun and happy - like something that children would listen to in their favorite tv shows.',\n",
       " 'prompt_aug': '',\n",
       " 'prompt_ch': 'The chord sequence is Bbmaj7, F7.',\n",
       " 'prompt_bt': 'The time signature is 4/4.',\n",
       " 'prompt_bpm': 'This song is played in Moderato.',\n",
       " 'prompt_key': 'The key of this song is Bb major.',\n",
       " 'beats': [[0.06,\n",
       "   0.62,\n",
       "   1.18,\n",
       "   1.76,\n",
       "   2.32,\n",
       "   2.88,\n",
       "   3.44,\n",
       "   4.0,\n",
       "   4.58,\n",
       "   5.14,\n",
       "   5.7,\n",
       "   6.26,\n",
       "   6.84,\n",
       "   7.4,\n",
       "   7.98,\n",
       "   8.52,\n",
       "   9.1,\n",
       "   9.66],\n",
       "  [2.0,\n",
       "   3.0,\n",
       "   4.0,\n",
       "   1.0,\n",
       "   2.0,\n",
       "   3.0,\n",
       "   4.0,\n",
       "   1.0,\n",
       "   2.0,\n",
       "   3.0,\n",
       "   4.0,\n",
       "   1.0,\n",
       "   2.0,\n",
       "   3.0,\n",
       "   4.0,\n",
       "   1.0,\n",
       "   2.0,\n",
       "   3.0]],\n",
       " 'bpm': 107.0,\n",
       " 'chords': ['Bbmaj7', 'F7'],\n",
       " 'chords_time': [0.464399092, 8.637823129000001],\n",
       " 'key': ['Bb', 'major'],\n",
       " 'keyprob': [0.783159077167511],\n",
       " 'is_audioset_eval_mcaps': True,\n",
       " 'input_ids': [100,\n",
       "  1082,\n",
       "  2324,\n",
       "  753,\n",
       "  3,\n",
       "  3844,\n",
       "  32,\n",
       "  208,\n",
       "  63,\n",
       "  13353,\n",
       "  9060,\n",
       "  6,\n",
       "  20513,\n",
       "  7,\n",
       "  6,\n",
       "  650,\n",
       "  8355,\n",
       "  27832,\n",
       "  6,\n",
       "  20747,\n",
       "  53,\n",
       "  11,\n",
       "  1148,\n",
       "  3,\n",
       "  9905,\n",
       "  3,\n",
       "  32,\n",
       "  6090,\n",
       "  1345,\n",
       "  1951,\n",
       "  6,\n",
       "  27536,\n",
       "  53,\n",
       "  8944,\n",
       "  52,\n",
       "  7,\n",
       "  6,\n",
       "  4035,\n",
       "  10105,\n",
       "  26,\n",
       "  3,\n",
       "  4651,\n",
       "  102,\n",
       "  7,\n",
       "  6,\n",
       "  1835,\n",
       "  4583,\n",
       "  8046,\n",
       "  11,\n",
       "  3,\n",
       "  3844,\n",
       "  32,\n",
       "  208,\n",
       "  63,\n",
       "  7981,\n",
       "  5,\n",
       "  94,\n",
       "  2993,\n",
       "  694,\n",
       "  11,\n",
       "  1095,\n",
       "  3,\n",
       "  18,\n",
       "  114,\n",
       "  424,\n",
       "  24,\n",
       "  502,\n",
       "  133,\n",
       "  3011,\n",
       "  12,\n",
       "  16,\n",
       "  70,\n",
       "  1305,\n",
       "  3,\n",
       "  17,\n",
       "  208,\n",
       "  1267,\n",
       "  5,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [[1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " 'input_values': [[[0.07256333529949188,\n",
       "    0.11293729394674301,\n",
       "    0.12868565320968628,\n",
       "    0.12479019910097122,\n",
       "    0.12120390683412552,\n",
       "    0.1271607130765915,\n",
       "    0.13296833634376526,\n",
       "    0.12674999237060547,\n",
       "    0.11014021188020706,\n",
       "    0.09320767223834991,\n",
       "    0.08040927350521088,\n",
       "    0.06827086955308914,\n",
       "    0.054111167788505554,\n",
       "    0.04032190889120102,\n",
       "    0.029847005382180214,\n",
       "    0.02262451872229576,\n",
       "    0.01808862015604973,\n",
       "    0.017526499927043915,\n",
       "    0.02171681821346283,\n",
       "    0.028292754665017128,\n",
       "    0.033415086567401886,\n",
       "    0.03523995727300644,\n",
       "    0.03505175560712814,\n",
       "    0.03616289794445038,\n",
       "    0.04189545661211014,\n",
       "    0.052502717822790146,\n",
       "    0.06307455152273178,\n",
       "    0.06666192412376404,\n",
       "    0.061354588717222214,\n",
       "    0.05289730057120323,\n",
       "    0.04856187850236893,\n",
       "    0.04966192692518234,\n",
       "    0.0518481507897377,\n",
       "    0.05121234431862831,\n",
       "    0.04747963696718216,\n",
       "    0.04235872998833656,\n",
       "    0.03834287077188492,\n",
       "    0.039186280220746994,\n",
       "    0.04752598702907562,\n",
       "    0.06007376313209534,\n",
       "    0.06817468255758286,\n",
       "    0.06619174778461456,\n",
       "    0.05806673318147659,\n",
       "    0.05296266824007034,\n",
       "    0.055103763937950134,\n",
       "    0.06030728295445442,\n",
       "    0.062009550631046295,\n",
       "    0.0578191913664341,\n",
       "    0.04966939985752106,\n",
       "    0.040367744863033295,\n",
       "    0.03176470845937729,\n",
       "    0.024641044437885284,\n",
       "    0.018456246703863144,\n",
       "    0.012001235038042068,\n",
       "    0.005783426575362682,\n",
       "    0.0027845092117786407,\n",
       "    0.004799750633537769,\n",
       "    0.008588832803070545,\n",
       "    0.008479628711938858,\n",
       "    0.0035032499581575394,\n",
       "    -0.0010965093970298767,\n",
       "    -0.002093401737511158,\n",
       "    -0.005983365699648857,\n",
       "    -0.02378544956445694,\n",
       "    -0.05675920844078064,\n",
       "    -0.09097947180271149,\n",
       "    -0.10983933508396149,\n",
       "    -0.11082050204277039,\n",
       "    -0.10625123977661133,\n",
       "    -0.10762752592563629,\n",
       "    -0.11372488737106323,\n",
       "    -0.11641804128885269,\n",
       "    -0.11399104446172714,\n",
       "    -0.11335082352161407,\n",
       "    -0.12008550763130188,\n",
       "    -0.13183394074440002,\n",
       "    -0.14290794730186462,\n",
       "    -0.1508193016052246,\n",
       "    -0.15531688928604126,\n",
       "    -0.15495476126670837,\n",
       "    -0.14951631426811218,\n",
       "    -0.14397364854812622,\n",
       "    -0.14446598291397095,\n",
       "    -0.149885356426239,\n",
       "    -0.1522504985332489,\n",
       "    -0.1469433307647705,\n",
       "    -0.13809895515441895,\n",
       "    -0.1313345581293106,\n",
       "    -0.12575159966945648,\n",
       "    -0.11785447597503662,\n",
       "    -0.11000021547079086,\n",
       "    -0.1080465316772461,\n",
       "    -0.11091393977403641,\n",
       "    -0.10968020558357239,\n",
       "    -0.0999947041273117,\n",
       "    -0.08982102572917938,\n",
       "    -0.0899459645152092,\n",
       "    -0.10022062808275223,\n",
       "    -0.11092057824134827,\n",
       "    -0.11617931723594666,\n",
       "    -0.11970168352127075,\n",
       "    -0.12627887725830078,\n",
       "    -0.13368409872055054,\n",
       "    -0.13657715916633606,\n",
       "    -0.13459821045398712,\n",
       "    -0.13201794028282166,\n",
       "    -0.13093246519565582,\n",
       "    -0.12897703051567078,\n",
       "    -0.12364829331636429,\n",
       "    -0.11571194976568222,\n",
       "    -0.10828196257352829,\n",
       "    -0.10541197657585144,\n",
       "    -0.1113564670085907,\n",
       "    -0.127036452293396,\n",
       "    -0.14562863111495972,\n",
       "    -0.15563687682151794,\n",
       "    -0.15204304456710815,\n",
       "    -0.1426127851009369,\n",
       "    -0.1396409422159195,\n",
       "    -0.14633037149906158,\n",
       "    -0.15467318892478943,\n",
       "    -0.1562064290046692,\n",
       "    -0.15077504515647888,\n",
       "    -0.1439642459154129,\n",
       "    -0.14018118381500244,\n",
       "    -0.1408146619796753,\n",
       "    -0.14585889875888824,\n",
       "    -0.15344718098640442,\n",
       "    -0.15937848389148712,\n",
       "    -0.16112202405929565,\n",
       "    -0.16200211644172668,\n",
       "    -0.16774004697799683,\n",
       "    -0.17832934856414795,\n",
       "    -0.18672461807727814,\n",
       "    -0.186753511428833,\n",
       "    -0.1792779564857483,\n",
       "    -0.16907989978790283,\n",
       "    -0.15917536616325378,\n",
       "    -0.15105147659778595,\n",
       "    -0.14727869629859924,\n",
       "    -0.14908656477928162,\n",
       "    -0.1525515466928482,\n",
       "    -0.1523076593875885,\n",
       "    -0.14954227209091187,\n",
       "    -0.15146152675151825,\n",
       "    -0.1610499620437622,\n",
       "    -0.17199718952178955,\n",
       "    -0.17709124088287354,\n",
       "    -0.17767027020454407,\n",
       "    -0.1797795593738556,\n",
       "    -0.18353866040706635,\n",
       "    -0.18291622400283813,\n",
       "    -0.17629143595695496,\n",
       "    -0.17066684365272522,\n",
       "    -0.17225228250026703,\n",
       "    -0.17755824327468872,\n",
       "    -0.17820942401885986,\n",
       "    -0.1716899275779724,\n",
       "    -0.1625821888446808,\n",
       "    -0.1549377590417862,\n",
       "    -0.14893987774848938,\n",
       "    -0.1451827436685562,\n",
       "    -0.14647124707698822,\n",
       "    -0.1527133733034134,\n",
       "    -0.15815472602844238,\n",
       "    -0.1575085073709488,\n",
       "    -0.15257523953914642,\n",
       "    -0.1490587592124939,\n",
       "    -0.14826753735542297,\n",
       "    -0.1461351215839386,\n",
       "    -0.14024467766284943,\n",
       "    -0.13343651592731476,\n",
       "    -0.1289827525615692,\n",
       "    -0.1259147822856903,\n",
       "    -0.12199198454618454,\n",
       "    -0.11872241646051407,\n",
       "    -0.11940419673919678,\n",
       "    -0.12272828072309494,\n",
       "    -0.12244510650634766,\n",
       "    -0.11517813801765442,\n",
       "    -0.10610371828079224,\n",
       "    -0.10413239151239395,\n",
       "    -0.11184990406036377,\n",
       "    -0.12225341796875,\n",
       "    -0.12582404911518097,\n",
       "    -0.11932604014873505,\n",
       "    -0.10686936229467392,\n",
       "    -0.09390074014663696,\n",
       "    -0.08252575993537903,\n",
       "    -0.07293593883514404,\n",
       "    -0.0667327269911766,\n",
       "    -0.06603240966796875,\n",
       "    -0.06959208846092224,\n",
       "    -0.07269372791051865,\n",
       "    -0.07246462255716324,\n",
       "    -0.0716836154460907,\n",
       "    -0.07515120506286621,\n",
       "    -0.08287540823221207,\n",
       "    -0.08909238874912262,\n",
       "    -0.08834760636091232,\n",
       "    -0.08073952049016953,\n",
       "    -0.07063622772693634,\n",
       "    -0.06214253976941109,\n",
       "    -0.05696208402514458,\n",
       "    -0.05480155348777771,\n",
       "    -0.05360289663076401,\n",
       "    -0.050148092210292816,\n",
       "    -0.042878374457359314,\n",
       "    -0.03450743854045868,\n",
       "    -0.030161283910274506,\n",
       "    -0.03229617327451706,\n",
       "    -0.03848685324192047,\n",
       "    -0.04439328610897064,\n",
       "    -0.04760158807039261,\n",
       "    -0.04838176816701889,\n",
       "    -0.048499271273612976,\n",
       "    -0.05021848529577255,\n",
       "    -0.054931603372097015,\n",
       "    -0.061478279531002045,\n",
       "    -0.06689624488353729,\n",
       "    -0.06974446773529053,\n",
       "    -0.07106097042560577,\n",
       "    -0.07091417163610458,\n",
       "    -0.06686963140964508,\n",
       "    -0.0591789074242115,\n",
       "    -0.055107370018959045,\n",
       "    -0.062223535031080246,\n",
       "    -0.0767965316772461,\n",
       "    -0.08508115261793137,\n",
       "    -0.07972976565361023,\n",
       "    -0.07004334777593613,\n",
       "    -0.07034485042095184,\n",
       "    -0.08138126134872437,\n",
       "    -0.09058213979005814,\n",
       "    -0.09006991237401962,\n",
       "    -0.08585149794816971,\n",
       "    -0.08592169731855392,\n",
       "    -0.08687600493431091,\n",
       "    -0.07939492166042328,\n",
       "    -0.06294210255146027,\n",
       "    -0.04670114070177078,\n",
       "    -0.03648476302623749,\n",
       "    -0.028587765991687775,\n",
       "    -0.01862604171037674,\n",
       "    -0.009343978017568588,\n",
       "    -0.004991279914975166,\n",
       "    -0.0030471105128526688,\n",
       "    0.0021892599761486053,\n",
       "    0.010310687124729156,\n",
       "    0.015741564333438873,\n",
       "    0.017643701285123825,\n",
       "    0.022360939532518387,\n",
       "    0.03389958664774895,\n",
       "    0.047330405563116074,\n",
       "    0.05557955801486969,\n",
       "    0.05934597924351692,\n",
       "    0.06548149138689041,\n",
       "    0.07676702737808228,\n",
       "    0.08793316036462784,\n",
       "    0.09266862273216248,\n",
       "    0.09135428816080093,\n",
       "    0.09003347903490067,\n",
       "    0.0930977389216423,\n",
       "    0.0984010100364685,\n",
       "    0.0995555967092514,\n",
       "    0.09313813596963882,\n",
       "    0.08379842340946198,\n",
       "    0.08075672388076782,\n",
       "    0.08746165037155151,\n",
       "    0.09603893011808395,\n",
       "    0.09519064426422119,\n",
       "    0.0834570974111557,\n",
       "    0.07091203331947327,\n",
       "    0.06673625856637955,\n",
       "    0.06935102492570877,\n",
       "    0.07157543301582336,\n",
       "    0.07136162370443344,\n",
       "    0.07205445319414139,\n",
       "    0.07390551269054413,\n",
       "    0.07178640365600586,\n",
       "    0.06253571063280106,\n",
       "    0.049498021602630615,\n",
       "    0.03747258335351944,\n",
       "    0.02739718183875084,\n",
       "    0.019105613231658936,\n",
       "    0.015448622405529022,\n",
       "    0.01891988515853882,\n",
       "    0.02621586248278618,\n",
       "    0.031220758333802223,\n",
       "    0.033055804669857025,\n",
       "    0.036618672311306,\n",
       "    0.044660523533821106,\n",
       "    0.05371537432074547,\n",
       "    0.059356700628995895,\n",
       "    0.06160774081945419,\n",
       "    0.0630519762635231,\n",
       "    0.06516589969396591,\n",
       "    0.06930581480264664,\n",
       "    0.07789625227451324,\n",
       "    0.09054800122976303,\n",
       "    0.10139936208724976,\n",
       "    0.10533034056425095,\n",
       "    0.10596230626106262,\n",
       "    0.11232325434684753,\n",
       "    0.1271670162677765,\n",
       "    0.14379438757896423,\n",
       "    0.15609145164489746,\n",
       "    0.1662546843290329,\n",
       "    0.1786310076713562,\n",
       "    0.19027355313301086,\n",
       "    0.19392210245132446,\n",
       "    0.18926936388015747,\n",
       "    0.18514186143875122,\n",
       "    0.18880008161067963,\n",
       "    0.19723880290985107,\n",
       "    0.20158450305461884,\n",
       "    0.1974489986896515,\n",
       "    0.1883794367313385,\n",
       "    0.18084369599819183,\n",
       "    0.17855359613895416,\n",
       "    0.1810971200466156,\n",
       "    0.18565696477890015,\n",
       "    0.18932385742664337,\n",
       "    0.1905178725719452,\n",
       "    0.18851464986801147,\n",
       "    0.18212825059890747,\n",
       "    0.1705862581729889,\n",
       "    0.1564747393131256,\n",
       "    0.14563529193401337,\n",
       "    0.14161169528961182,\n",
       "    0.1409272998571396,\n",
       "    0.1362943798303604,\n",
       "    0.12438911199569702,\n",
       "    0.10832798480987549,\n",
       "    0.09283919632434845,\n",
       "    0.08016922324895859,\n",
       "    0.07158467173576355,\n",
       "    0.06932710111141205,\n",
       "    0.07402005046606064,\n",
       "    0.08176867663860321,\n",
       "    0.08698496222496033,\n",
       "    0.08788687735795975,\n",
       "    0.08700168132781982,\n",
       "    0.08662918955087662,\n",
       "    0.08645429462194443,\n",
       "    0.08596824109554291,\n",
       "    0.08649462461471558,\n",
       "    0.089300736784935,\n",
       "    0.09349251538515091,\n",
       "    0.09751853346824646,\n",
       "    0.10164192318916321,\n",
       "    0.10713054984807968,\n",
       "    0.11350249499082565,\n",
       "    0.11825180798768997,\n",
       "    0.1195029690861702,\n",
       "    0.11804813891649246,\n",
       "    0.11676107347011566,\n",
       "    0.11851560324430466,\n",
       "    0.12408575415611267,\n",
       "    0.13108079135417938,\n",
       "    0.13540397584438324,\n",
       "    0.13494998216629028,\n",
       "    0.13153360784053802,\n",
       "    0.12798947095870972,\n",
       "    0.12420424073934555,\n",
       "    0.11842331290245056,\n",
       "    0.11242763698101044,\n",
       "    0.11194255948066711,\n",
       "    0.11972174048423767,\n",
       "    0.13067121803760529,\n",
       "    0.13723337650299072,\n",
       "    0.1386205106973648,\n",
       "    0.14087674021720886,\n",
       "    0.1473848968744278,\n",
       "    0.15346135199069977,\n",
       "    0.1529209017753601,\n",
       "    0.14692743122577667,\n",
       "    0.1420615315437317,\n",
       "    0.14051319658756256,\n",
       "    0.13701608777046204,\n",
       "    0.1272525042295456,\n",
       "    0.11552828550338745,\n",
       "    0.11035289615392685,\n",
       "    0.11386337131261826,\n",
       "    0.11972334235906601,\n",
       "    0.12198002636432648,\n",
       "    0.1221856102347374,\n",
       "    0.12537474930286407,\n",
       "    0.13166308403015137,\n",
       "    0.13572457432746887,\n",
       "    0.13414202630519867,\n",
       "    0.12935441732406616,\n",
       "    0.12483258545398712,\n",
       "    0.11941096186637878,\n",
       "    0.10935750603675842,\n",
       "    0.09467107057571411,\n",
       "    0.08009680360555649,\n",
       "    0.06984870880842209,\n",
       "    0.06403715908527374,\n",
       "    0.060872651636600494,\n",
       "    0.059558991342782974,\n",
       "    0.05935950204730034,\n",
       "    0.05830051004886627,\n",
       "    0.055786602199077606,\n",
       "    0.05546783283352852,\n",
       "    0.06227460503578186,\n",
       "    0.07615602761507034,\n",
       "    0.09115606546401978,\n",
       "    0.10142382234334946,\n",
       "    0.10603448003530502,\n",
       "    0.10710498690605164,\n",
       "    0.1058994010090828,\n",
       "    0.10309763252735138,\n",
       "    0.1009737029671669,\n",
       "    0.1018609032034874,\n",
       "    0.10442043095827103,\n",
       "    0.10413020849227905,\n",
       "    0.09833412617444992,\n",
       "    0.08917055279016495,\n",
       "    0.08111224323511124,\n",
       "    0.07742257416248322,\n",
       "    0.07930243015289307,\n",
       "    0.08569590002298355,\n",
       "    0.09214407205581665,\n",
       "    0.09228083491325378,\n",
       "    0.08369921147823334,\n",
       "    0.07166104018688202,\n",
       "    0.06416027992963791,\n",
       "    0.06334759294986725,\n",
       "    0.06431201100349426,\n",
       "    0.06214139237999916,\n",
       "    0.05677435174584389,\n",
       "    0.0499798022210598,\n",
       "    0.04159940406680107,\n",
       "    0.0320209339261055,\n",
       "    0.025635890662670135,\n",
       "    0.0271223783493042,\n",
       "    0.034620366990566254,\n",
       "    0.04083964228630066,\n",
       "    0.04189586266875267,\n",
       "    0.04134505242109299,\n",
       "    0.043702393770217896,\n",
       "    0.047551609575748444,\n",
       "    0.048298101872205734,\n",
       "    0.04516192153096199,\n",
       "    0.04113972187042236,\n",
       "    0.03720375522971153,\n",
       "    0.03155910596251488,\n",
       "    0.025358520448207855,\n",
       "    0.023985013365745544,\n",
       "    0.029480356723070145,\n",
       "    0.03546372428536415,\n",
       "    0.034455060958862305,\n",
       "    0.028773900121450424,\n",
       "    0.028806935995817184,\n",
       "    0.03953219950199127,\n",
       "    0.0534692257642746,\n",
       "    0.059906478971242905,\n",
       "    0.057949889451265335,\n",
       "    0.05635886266827583,\n",
       "    0.06167927756905556,\n",
       "    0.07086071372032166,\n",
       "    0.07638224214315414,\n",
       "    0.07506056129932404,\n",
       "    0.06949150562286377,\n",
       "    0.06309214979410172,\n",
       "    0.057034894824028015,\n",
       "    0.05187937617301941,\n",
       "    0.04937909543514252,\n",
       "    0.051170751452445984,\n",
       "    0.056670501828193665,\n",
       "    0.06343378871679306,\n",
       "    0.06936590373516083,\n",
       "    0.07396901398897171,\n",
       "    0.07766912877559662,\n",
       "    0.08064372092485428,\n",
       "    0.082546666264534,\n",
       "    0.0830128937959671,\n",
       "    0.08191407471895218,\n",
       "    0.07885808497667313,\n",
       "    0.07280851900577545,\n",
       "    0.06318391859531403,\n",
       "    0.05188775062561035,\n",
       "    0.043237097561359406,\n",
       "    0.04006032273173332,\n",
       "    0.039557114243507385,\n",
       "    0.034922532737255096,\n",
       "    0.022814078256487846,\n",
       "    0.008472445420920849,\n",
       "    0.0013469886034727097,\n",
       "    0.005139114335179329,\n",
       "    0.013683744706213474,\n",
       "    0.017626523971557617,\n",
       "    0.014053326100111008,\n",
       "    0.00819146167486906,\n",
       "    0.006337584927678108,\n",
       "    0.009081633761525154,\n",
       "    0.012013763189315796,\n",
       "    0.011456127278506756,\n",
       "    0.007846686989068985,\n",
       "    0.0038034766912460327,\n",
       "    0.0005727224051952362,\n",
       "    -0.002553507685661316,\n",
       "    -0.005878942087292671,\n",
       "    -0.007698904722929001,\n",
       "    -0.005734037607908249,\n",
       "    0.00015928223729133606,\n",
       "    0.007244940847158432,\n",
       "    0.011836159974336624,\n",
       "    0.011728007346391678,\n",
       "    0.007235076278448105,\n",
       "    0.0009823963046073914,\n",
       "    -0.003021255135536194,\n",
       "    -0.0014332681894302368,\n",
       "    0.005765356123447418,\n",
       "    0.014739908277988434,\n",
       "    0.021092623472213745,\n",
       "    0.02397933043539524,\n",
       "    0.026268165558576584,\n",
       "    0.030777255073189735,\n",
       "    0.03763800859451294,\n",
       "    0.045510172843933105,\n",
       "    0.0534851998090744,\n",
       "    0.060584086924791336,\n",
       "    0.06516914069652557,\n",
       "    0.06719459593296051,\n",
       "    0.07008296996355057,\n",
       "    0.07723935693502426,\n",
       "    0.08619897067546844,\n",
       "    0.08924545347690582,\n",
       "    0.08173812925815582,\n",
       "    0.06789085268974304,\n",
       "    0.0561104491353035,\n",
       "    0.05001707002520561,\n",
       "    0.04661141708493233,\n",
       "    0.04228084906935692,\n",
       "    0.037128668278455734,\n",
       "    0.03280138969421387,\n",
       "    0.029186591506004333,\n",
       "    0.025095239281654358,\n",
       "    0.020311910659074783,\n",
       "    0.015255352482199669,\n",
       "    0.010115545243024826,\n",
       "    0.005806758999824524,\n",
       "    0.003768129274249077,\n",
       "    0.0025299042463302612,\n",
       "    -0.003308214247226715,\n",
       "    -0.016570165753364563,\n",
       "    -0.030721349641680717,\n",
       "    -0.03481704741716385,\n",
       "    -0.027137231081724167,\n",
       "    -0.019469190388917923,\n",
       "    -0.023982688784599304,\n",
       "    -0.03858555108308792,\n",
       "    -0.04998883232474327,\n",
       "    -0.05009322613477707,\n",
       "    -0.04397796094417572,\n",
       "    -0.04077363386750221,\n",
       "    -0.04189532250165939,\n",
       "    -0.04177400469779968,\n",
       "    -0.036904226988554,\n",
       "    -0.029551174491643906,\n",
       "    -0.023426063358783722,\n",
       "    -0.0198320634663105,\n",
       "    -0.0185994915664196,\n",
       "    -0.01944584771990776,\n",
       "    -0.0209182295948267,\n",
       "    -0.020282577723264694,\n",
       "    -0.01613241620361805,\n",
       "    -0.009677992202341557,\n",
       "    -0.002359353005886078,\n",
       "    0.005919456481933594,\n",
       "    0.014564825221896172,\n",
       "    0.020460646599531174,\n",
       "    0.021416477859020233,\n",
       "    0.020535582676529884,\n",
       "    0.023596297949552536,\n",
       "    0.031528737396001816,\n",
       "    0.03869025409221649,\n",
       "    0.04013832286000252,\n",
       "    0.03781760111451149,\n",
       "    0.03700960427522659,\n",
       "    0.038750261068344116,\n",
       "    0.03933967277407646,\n",
       "    0.036720894277095795,\n",
       "    0.03345530107617378,\n",
       "    0.032369304448366165,\n",
       "    0.03259100019931793,\n",
       "    0.03211633116006851,\n",
       "    0.03179638832807541,\n",
       "    0.033494144678115845,\n",
       "    0.035328298807144165,\n",
       "    0.03239591419696808,\n",
       "    0.022942766547203064,\n",
       "    0.0110277459025383,\n",
       "    0.0015658978372812271,\n",
       "    -0.005287397652864456,\n",
       "    -0.013072732836008072,\n",
       "    -0.024043060839176178,\n",
       "    -0.03763780742883682,\n",
       "    -0.052919089794158936,\n",
       "    -0.06936541199684143,\n",
       "    -0.08468777686357498,\n",
       "    -0.09476962685585022,\n",
       "    -0.09786532819271088,\n",
       "    -0.09739088267087936,\n",
       "    -0.09833499789237976,\n",
       "    -0.10156095027923584,\n",
       "    -0.10393284261226654,\n",
       "    -0.10402315109968185,\n",
       "    -0.1050233468413353,\n",
       "    -0.11057856678962708,\n",
       "    -0.11958393454551697,\n",
       "    -0.12735413014888763,\n",
       "    -0.13130268454551697,\n",
       "    -0.13316139578819275,\n",
       "    -0.13532811403274536,\n",
       "    -0.1373007446527481,\n",
       "    -0.13713368773460388,\n",
       "    -0.1349254548549652,\n",
       "    -0.1328243762254715,\n",
       "    -0.13201765716075897,\n",
       "    -0.1316409409046173,\n",
       "    -0.1307806521654129,\n",
       "    -0.12958990037441254,\n",
       "    -0.12770360708236694,\n",
       "    -0.12364411354064941,\n",
       "    -0.11771082878112793,\n",
       "    -0.11411944031715393,\n",
       "    -0.11752286553382874,\n",
       "    -0.1272730827331543,\n",
       "    -0.1374347060918808,\n",
       "    -0.14308388531208038,\n",
       "    -0.14433787763118744,\n",
       "    -0.14323002099990845,\n",
       "    -0.13975009322166443,\n",
       "    -0.13411583006381989,\n",
       "    -0.1308377981185913,\n",
       "    -0.13535213470458984,\n",
       "    -0.14559973776340485,\n",
       "    -0.15148480236530304,\n",
       "    -0.14601922035217285,\n",
       "    -0.1344626247882843,\n",
       "    -0.12852978706359863,\n",
       "    -0.13265979290008545,\n",
       "    -0.14013800024986267,\n",
       "    -0.142881378531456,\n",
       "    -0.1405605971813202,\n",
       "    -0.1380346417427063,\n",
       "    -0.1376343071460724,\n",
       "    -0.13790374994277954,\n",
       "    -0.13807159662246704,\n",
       "    -0.13925805687904358,\n",
       "    -0.14109128713607788,\n",
       "    -0.14104491472244263,\n",
       "    -0.13860882818698883,\n",
       "    -0.13706228137016296,\n",
       "    -0.13878965377807617,\n",
       "    -0.14098307490348816,\n",
       "    -0.1390264332294464,\n",
       "    -0.1334003210067749,\n",
       "    -0.13033272325992584,\n",
       "    -0.13518521189689636,\n",
       "    -0.14723122119903564,\n",
       "    -0.16141441464424133,\n",
       "    -0.17311955988407135,\n",
       "    -0.1800781488418579,\n",
       "    -0.18155664205551147,\n",
       "    -0.17807498574256897,\n",
       "    -0.17175057530403137,\n",
       "    -0.16524896025657654,\n",
       "    -0.15991473197937012,\n",
       "    -0.15551993250846863,\n",
       "    -0.15155154466629028,\n",
       "    -0.14764301478862762,\n",
       "    -0.14282043278217316,\n",
       "    -0.13589443266391754,\n",
       "    -0.12757033109664917,\n",
       "    -0.121308833360672,\n",
       "    -0.12072129547595978,\n",
       "    -0.12585195899009705,\n",
       "    -0.13264384865760803,\n",
       "    -0.13641692698001862,\n",
       "    -0.13579264283180237,\n",
       "    -0.13329678773880005,\n",
       "    -0.13248194754123688,\n",
       "    -0.13464781641960144,\n",
       "    -0.13817912340164185,\n",
       "    -0.14068171381950378,\n",
       "    -0.14139261841773987,\n",
       "    -0.14148344099521637,\n",
       "    -0.1425808221101761,\n",
       "    -0.1456569880247116,\n",
       "    -0.15110653638839722,\n",
       "    -0.1586633324623108,\n",
       "    -0.16651184856891632,\n",
       "    -0.17144648730754852,\n",
       "    -0.17152969539165497,\n",
       "    -0.16885358095169067,\n",
       "    -0.1683909296989441,\n",
       "    -0.1731279492378235,\n",
       "    -0.1807262897491455,\n",
       "    -0.18566548824310303,\n",
       "    -0.1843298226594925,\n",
       "    -0.17731234431266785,\n",
       "    -0.16732725501060486,\n",
       "    -0.1565786898136139,\n",
       "    -0.14686748385429382,\n",
       "    -0.140664204955101,\n",
       "    -0.1396908462047577,\n",
       "    -0.14192700386047363,\n",
       "    -0.14182385802268982,\n",
       "    -0.13532651960849762,\n",
       "    -0.12446620315313339,\n",
       "    -0.11546584963798523,\n",
       "    -0.11194972693920135,\n",
       "    -0.11140254139900208,\n",
       "    -0.10913459956645966,\n",
       "    -0.10423426330089569,\n",
       "    -0.09966009855270386,\n",
       "    -0.09715120494365692,\n",
       "    -0.09480591118335724,\n",
       "    -0.09056281298398972,\n",
       "    -0.08554767817258835,\n",
       "    -0.08167752623558044,\n",
       "    -0.07746582478284836,\n",
       "    -0.06947726011276245,\n",
       "    -0.05815979093313217,\n",
       "    -0.04914296045899391,\n",
       "    -0.047048743814229965,\n",
       "    -0.05004112422466278,\n",
       "    -0.05267571285367012,\n",
       "    -0.05277034640312195,\n",
       "    -0.052680376917123795,\n",
       "    -0.05416573956608772,\n",
       "    -0.05521470308303833,\n",
       "    -0.05322301760315895,\n",
       "    -0.04886947572231293,\n",
       "    -0.044527485966682434,\n",
       "    -0.04007968679070473,\n",
       "    -0.03307604417204857,\n",
       "    -0.023302368819713593,\n",
       "    -0.014900177717208862,\n",
       "    -0.012494057416915894,\n",
       "    -0.016103282570838928,\n",
       "    -0.021146103739738464,\n",
       "    -0.023266568779945374,\n",
       "    -0.022313036024570465,\n",
       "    -0.021714545786380768,\n",
       "    -0.02493591606616974,\n",
       "    -0.03291337937116623,\n",
       "    -0.04407116770744324,\n",
       "    -0.055556103587150574,\n",
       "    -0.06407269090414047,\n",
       "    -0.06657575815916061,\n",
       "    -0.0619097463786602,\n",
       "    -0.052717424929142,\n",
       "    -0.04503351077437401,\n",
       "    -0.04445004463195801,\n",
       "    -0.05171143636107445,\n",
       "    -0.06199631094932556,\n",
       "    -0.06863558292388916,\n",
       "    -0.06779347360134125,\n",
       "    -0.06053900718688965,\n",
       "    -0.05165424570441246,\n",
       "    -0.046332329511642456,\n",
       "    -0.04633538797497749,\n",
       "    -0.04828062653541565,\n",
       "    -0.046715397387742996,\n",
       "    -0.04025263339281082,\n",
       "    -0.0340007022023201,\n",
       "    -0.034155696630477905,\n",
       "    -0.040221136063337326,\n",
       "    -0.044471435248851776,\n",
       "    -0.039853811264038086,\n",
       "    -0.02694767341017723,\n",
       "    -0.012242890894412994,\n",
       "    -0.0012445375323295593,\n",
       "    0.004982791841030121,\n",
       "    0.007873542606830597,\n",
       "    0.009021814912557602,\n",
       "    0.010166088119149208,\n",
       "    0.012953519821166992,\n",
       "    0.017278142273426056,\n",
       "    0.021437397226691246,\n",
       "    0.025407342240214348,\n",
       "    0.03239759802818298,\n",
       "    0.044782668352127075,\n",
       "    0.059214577078819275,\n",
       "    0.06885217130184174,\n",
       "    0.07140177488327026,\n",
       "    0.07244808971881866,\n",
       "    0.07874403893947601,\n",
       "    0.08979639410972595,\n",
       "    0.09899045526981354,\n",
       "    0.10273069888353348,\n",
       "    0.10501985251903534,\n",
       "    0.11116459965705872,\n",
       "    0.1194666251540184,\n",
       "    0.12276731431484222,\n",
       "    0.11814441531896591,\n",
       "    0.11152724921703339,\n",
       "    0.11055268347263336,\n",
       "    0.11487116664648056,\n",
       "    0.11665261536836624,\n",
       "    0.11045119911432266,\n",
       "    0.09944014251232147,\n",
       "    0.09053804725408554,\n",
       "    0.08586500585079193,\n",
       "    0.08178278803825378,\n",
       "    0.07535681873559952,\n",
       "    0.06829161942005157,\n",
       "    0.06331764906644821,\n",
       "    0.05923781916499138,\n",
       "    0.05215822532773018,\n",
       "    0.04106227308511734,\n",
       "    0.029875515028834343,\n",
       "    0.02332065999507904,\n",
       "    0.022032473236322403,\n",
       "    0.022724661976099014,\n",
       "    0.022525373846292496,\n",
       "    0.022042110562324524,\n",
       "    0.023950912058353424,\n",
       "    0.02921002358198166,\n",
       "    0.03543796390295029,\n",
       "    0.03941398859024048,\n",
       "    0.04079357534646988,\n",
       "    0.042348481714725494,\n",
       "    0.046152032911777496,\n",
       "    0.05071267858147621,\n",
       "    0.0533040314912796,\n",
       "    0.05449369177222252,\n",
       "    0.058115966618061066,\n",
       "    0.06611470878124237,\n",
       "    0.07555100321769714,\n",
       "    0.08246561139822006,\n",
       "    0.087198905646801,\n",
       "    0.09332416951656342,\n",
       "    0.10179701447486877,\n",
       "    0.1091153472661972,\n",
       "    0.1121303141117096,\n",
       "    0.11216221749782562,\n",
       "    0.11263273656368256,\n",
       "    0.11459106206893921,\n",
       "    0.11704125255346298,\n",
       "    0.1204262301325798,\n",
       "    0.12642940878868103,\n",
       "    0.13389310240745544,\n",
       "    0.13797329366207123,\n",
       "    0.13530851900577545,\n",
       "    0.12853829562664032,\n",
       "    0.12376294285058975,\n",
       "    0.12411507219076157,\n",
       "    0.12729640305042267,\n",
       "    0.12896883487701416,\n",
       "    0.1270928829908371,\n",
       "    0.12319636344909668,\n",
       "    0.12092767655849457,\n",
       "    0.1233871579170227,\n",
       "    0.1302603930234909,\n",
       "    0.13714002072811127,\n",
       "    0.13918715715408325,\n",
       "    0.13623768091201782,\n",
       "    0.1329345703125,\n",
       "    0.13280417025089264,\n",
       "    0.13361391425132751,\n",
       "    0.130258247256279,\n",
       "    0.12124629318714142,\n",
       "    0.11009742319583893,\n",
       "    0.10050847381353378,\n",
       "    0.09263138473033905,\n",
       "    0.08503757417201996,\n",
       "    0.0782361775636673,\n",
       "    0.07431451231241226,\n",
       "    0.07398565113544464,\n",
       "    0.07574272155761719,\n",
       "    0.07759517431259155,\n",
       "    0.07811640202999115,\n",
       "    0.07596906274557114,\n",
       "    0.07024592161178589,\n",
       "    0.062062300741672516,\n",
       "    0.054753705859184265,\n",
       "    0.05164562538266182,\n",
       "    0.0539311058819294,\n",
       "    0.060629162937402725,\n",
       "    0.06940975040197372,\n",
       "    0.07709581404924393,\n",
       "    0.08102302253246307,\n",
       "    0.08141341060400009,\n",
       "    0.08155013620853424,\n",
       "    0.08402134478092194,\n",
       "    0.08735448122024536,\n",
       "    0.08819039911031723,\n",
       "    0.08648700267076492,\n",
       "    0.08612483739852905,\n",
       "    0.08961369097232819,\n",
       "    0.09460142254829407,\n",
       "    0.09742771834135056,\n",
       "    0.09820745885372162,\n",
       "    0.09953929483890533,\n",
       "    0.10104292631149292,\n",
       "    0.09885867685079575,\n",
       "    0.09173543751239777,\n",
       "    0.08463717252016068,\n",
       "    0.0836501270532608,\n",
       "    0.08861199021339417,\n",
       "    0.09350341558456421,\n",
       "    0.09388799220323563,\n",
       "    0.09134244173765182,\n",
       "    0.0898679792881012,\n",
       "    0.0903889536857605,\n",
       "    0.09073935449123383,\n",
       "    0.08975693583488464,\n",
       "    0.08889877796173096,\n",
       "    0.08959434926509857,\n",
       "    0.09085235744714737,\n",
       "    0.09049013257026672,\n",
       "    0.08786498755216599,\n",
       "    0.08449747413396835,\n",
       "    0.0824253261089325,\n",
       "    0.08261679112911224,\n",
       "    0.08457617461681366,\n",
       "    0.08660337328910828,\n",
       "    0.08652634918689728,\n",
       "    0.08319610357284546,\n",
       "    0.07780824601650238,\n",
       "    0.07326801121234894,\n",
       "    0.07167734205722809,\n",
       "    0.07254435122013092,\n",
       "    0.07368218153715134,\n",
       "    0.07345248013734818,\n",
       "    0.07158908993005753,\n",
       "    0.068239726126194,\n",
       "    0.06339305639266968,\n",
       "    0.057707205414772034,\n",
       "    0.05293060466647148,\n",
       "    0.05038509517908096,\n",
       "    0.0493006557226181,\n",
       "    0.04782143980264664,\n",
       "    0.04589848220348358,\n",
       "    0.04604963958263397,\n",
       "    0.05042294040322304,\n",
       "    0.05783310532569885,\n",
       "    0.06483770906925201,\n",
       "    0.06945484131574631,\n",
       "    0.07250135391950607,\n",
       "    0.0750466138124466,\n",
       "    0.07602837681770325,\n",
       "    0.0735732764005661,\n",
       "    0.06796010583639145,\n",
       "    0.06183366850018501,\n",
       "    0.05742670223116875,\n",
       "    0.054712627083063126,\n",
       "    0.05260203778743744,\n",
       "    0.05075627192854881,\n",
       "    0.049225497990846634,\n",
       "    0.04693044722080231,\n",
       "    0.041816264390945435,\n",
       "    0.03281179070472717,\n",
       "    0.02100793644785881,\n",
       "    0.008692371658980846,\n",
       "    -0.002268926240503788,\n",
       "    -0.010981675237417221,\n",
       "    -0.017299961298704147,\n",
       "    -0.02185475453734398,\n",
       "    -0.026062583550810814,\n",
       "    -0.031246531754732132,\n",
       "    -0.037331610918045044,\n",
       "    -0.04263587296009064,\n",
       "    -0.045195866376161575,\n",
       "    -0.04430948942899704,\n",
       "    -0.04089862480759621,\n",
       "    -0.03668350726366043,\n",
       "    -0.03304511308670044,\n",
       "    -0.03024224005639553,\n",
       "    -0.02733949013054371,\n",
       "    -0.023101944476366043,\n",
       "    -0.01742113009095192,\n",
       "    -0.011841190978884697,\n",
       "    -0.008188808336853981,\n",
       "    -0.006499655079096556,\n",
       "    -0.004729131702333689,\n",
       "    -0.0008857822977006435,\n",
       "    0.004812540486454964,\n",
       "    0.010254636406898499,\n",
       "    0.013560970313847065,\n",
       "    0.014312239363789558,\n",
       "    0.013015217147767544,\n",
       "    0.010528617538511753,\n",
       "    0.008398517966270447,\n",
       "    0.008576009422540665,\n",
       "    0.0116354338824749,\n",
       "    0.015837259590625763,\n",
       "    0.019293170422315598,\n",
       "    0.022829461842775345,\n",
       "    0.029025737196207047,\n",
       "    0.037990957498550415,\n",
       "    0.04590579867362976,\n",
       "    0.049107328057289124,\n",
       "    0.04828789830207825,\n",
       "    ...]]],\n",
       " 'padding_mask': [[1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   ...]]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 512])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [4, 12, 512, 512] doesn't match the broadcast shape [4, 4, 12, 512, 512]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m trainer.train()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\NLP2\\Lib\\site-packages\\transformers\\trainer.py:2240\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2238\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2239\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[32m   2241\u001b[39m         args=args,\n\u001b[32m   2242\u001b[39m         resume_from_checkpoint=resume_from_checkpoint,\n\u001b[32m   2243\u001b[39m         trial=trial,\n\u001b[32m   2244\u001b[39m         ignore_keys_for_eval=ignore_keys_for_eval,\n\u001b[32m   2245\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\NLP2\\Lib\\site-packages\\transformers\\trainer.py:2555\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2548\u001b[39m context = (\n\u001b[32m   2549\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2550\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2551\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2552\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2553\u001b[39m )\n\u001b[32m   2554\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2555\u001b[39m     tr_loss_step = \u001b[38;5;28mself\u001b[39m.training_step(model, inputs, num_items_in_batch)\n\u001b[32m   2557\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2558\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2559\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2560\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2561\u001b[39m ):\n\u001b[32m   2562\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2563\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\NLP2\\Lib\\site-packages\\transformers\\trainer.py:3745\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3742\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3744\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3745\u001b[39m     loss = \u001b[38;5;28mself\u001b[39m.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n\u001b[32m   3747\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3749\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3750\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3751\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\NLP2\\Lib\\site-packages\\transformers\\trainer.py:3810\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3808\u001b[39m         loss_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   3809\u001b[39m     inputs = {**inputs, **loss_kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m3810\u001b[39m outputs = model(**inputs)\n\u001b[32m   3811\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3812\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\NLP2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\NLP2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\NLP2\\Lib\\site-packages\\accelerate\\utils\\operations.py:818\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    817\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m818\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model_forward(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\NLP2\\Lib\\site-packages\\accelerate\\utils\\operations.py:806\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    805\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m806\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28mself\u001b[39m.model_forward(*args, **kwargs))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\NLP2\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\NLP2\\Lib\\site-packages\\peft\\peft_model.py:1757\u001b[39m, in \u001b[36mPeftModelForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[39m\n\u001b[32m   1755\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(**kwargs):\n\u001b[32m   1756\u001b[39m         kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m-> \u001b[39m\u001b[32m1757\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.base_model(\n\u001b[32m   1758\u001b[39m             input_ids=input_ids,\n\u001b[32m   1759\u001b[39m             attention_mask=attention_mask,\n\u001b[32m   1760\u001b[39m             inputs_embeds=inputs_embeds,\n\u001b[32m   1761\u001b[39m             labels=labels,\n\u001b[32m   1762\u001b[39m             output_attentions=output_attentions,\n\u001b[32m   1763\u001b[39m             output_hidden_states=output_hidden_states,\n\u001b[32m   1764\u001b[39m             return_dict=return_dict,\n\u001b[32m   1765\u001b[39m             **kwargs,\n\u001b[32m   1766\u001b[39m         )\n\u001b[32m   1768\u001b[39m batch_size = _get_batch_size(input_ids, inputs_embeds)\n\u001b[32m   1769\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1770\u001b[39m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\NLP2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\NLP2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\NLP2\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:193\u001b[39m, in \u001b[36mBaseTuner.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any):\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.forward(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\NLP2\\Lib\\site-packages\\transformers\\models\\musicgen\\modeling_musicgen.py:1996\u001b[39m, in \u001b[36mMusicgenForConditionalGeneration.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, input_values, padding_mask, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m   1991\u001b[39m kwargs_decoder = {\n\u001b[32m   1992\u001b[39m     argument[\u001b[38;5;28mlen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mdecoder_\u001b[39m\u001b[33m\"\u001b[39m) :]: value \u001b[38;5;28;01mfor\u001b[39;00m argument, value \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m argument.startswith(\u001b[33m\"\u001b[39m\u001b[33mdecoder_\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1993\u001b[39m }\n\u001b[32m   1995\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1996\u001b[39m     encoder_outputs = \u001b[38;5;28mself\u001b[39m.text_encoder(\n\u001b[32m   1997\u001b[39m         input_ids=input_ids,\n\u001b[32m   1998\u001b[39m         attention_mask=attention_mask,\n\u001b[32m   1999\u001b[39m         inputs_embeds=inputs_embeds,\n\u001b[32m   2000\u001b[39m         output_attentions=output_attentions,\n\u001b[32m   2001\u001b[39m         output_hidden_states=output_hidden_states,\n\u001b[32m   2002\u001b[39m         return_dict=return_dict,\n\u001b[32m   2003\u001b[39m         **kwargs_text_encoder,\n\u001b[32m   2004\u001b[39m     )\n\u001b[32m   2005\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m   2006\u001b[39m     encoder_outputs = BaseModelOutput(*encoder_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\NLP2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\NLP2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\NLP2\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:2010\u001b[39m, in \u001b[36mT5EncoderModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1985\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1986\u001b[39m \u001b[33;03minput_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\u001b[39;00m\n\u001b[32m   1987\u001b[39m \u001b[33;03m    Indices of input sequence tokens in the vocabulary. T5 is a model with relative position embeddings so you\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2006\u001b[39m \u001b[33;03m>>> last_hidden_states = outputs.last_hidden_state\u001b[39;00m\n\u001b[32m   2007\u001b[39m \u001b[33;03m```\"\"\"\u001b[39;00m\n\u001b[32m   2008\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m2010\u001b[39m encoder_outputs = \u001b[38;5;28mself\u001b[39m.encoder(\n\u001b[32m   2011\u001b[39m     input_ids=input_ids,\n\u001b[32m   2012\u001b[39m     attention_mask=attention_mask,\n\u001b[32m   2013\u001b[39m     inputs_embeds=inputs_embeds,\n\u001b[32m   2014\u001b[39m     head_mask=head_mask,\n\u001b[32m   2015\u001b[39m     output_attentions=output_attentions,\n\u001b[32m   2016\u001b[39m     output_hidden_states=output_hidden_states,\n\u001b[32m   2017\u001b[39m     return_dict=return_dict,\n\u001b[32m   2018\u001b[39m )\n\u001b[32m   2020\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m encoder_outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\NLP2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\NLP2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\NLP2\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1125\u001b[39m, in \u001b[36mT5Stack.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m   1108\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m   1109\u001b[39m         layer_module.forward,\n\u001b[32m   1110\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1122\u001b[39m         cache_position,\n\u001b[32m   1123\u001b[39m     )\n\u001b[32m   1124\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1125\u001b[39m     layer_outputs = layer_module(\n\u001b[32m   1126\u001b[39m         hidden_states,\n\u001b[32m   1127\u001b[39m         attention_mask=causal_mask,\n\u001b[32m   1128\u001b[39m         position_bias=position_bias,\n\u001b[32m   1129\u001b[39m         encoder_hidden_states=encoder_hidden_states,\n\u001b[32m   1130\u001b[39m         encoder_attention_mask=encoder_extended_attention_mask,\n\u001b[32m   1131\u001b[39m         encoder_decoder_position_bias=encoder_decoder_position_bias,\n\u001b[32m   1132\u001b[39m         layer_head_mask=layer_head_mask,\n\u001b[32m   1133\u001b[39m         cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n\u001b[32m   1134\u001b[39m         past_key_value=past_key_values,\n\u001b[32m   1135\u001b[39m         use_cache=use_cache,\n\u001b[32m   1136\u001b[39m         output_attentions=output_attentions,\n\u001b[32m   1137\u001b[39m         return_dict=return_dict,\n\u001b[32m   1138\u001b[39m         cache_position=cache_position,\n\u001b[32m   1139\u001b[39m     )\n\u001b[32m   1141\u001b[39m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[32m   1142\u001b[39m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[32m   1143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\NLP2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\NLP2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\NLP2\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:679\u001b[39m, in \u001b[36mT5Block.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict, cache_position)\u001b[39m\n\u001b[32m    663\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    664\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    665\u001b[39m     hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    677\u001b[39m     cache_position=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    678\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m679\u001b[39m     self_attention_outputs = \u001b[38;5;28mself\u001b[39m.layer[\u001b[32m0\u001b[39m](\n\u001b[32m    680\u001b[39m         hidden_states,\n\u001b[32m    681\u001b[39m         attention_mask=attention_mask,\n\u001b[32m    682\u001b[39m         position_bias=position_bias,\n\u001b[32m    683\u001b[39m         layer_head_mask=layer_head_mask,\n\u001b[32m    684\u001b[39m         past_key_value=past_key_value,\n\u001b[32m    685\u001b[39m         use_cache=use_cache,\n\u001b[32m    686\u001b[39m         output_attentions=output_attentions,\n\u001b[32m    687\u001b[39m         cache_position=cache_position,\n\u001b[32m    688\u001b[39m     )\n\u001b[32m    689\u001b[39m     hidden_states, past_key_value = self_attention_outputs[:\u001b[32m2\u001b[39m]\n\u001b[32m    690\u001b[39m     attention_outputs = self_attention_outputs[\u001b[32m2\u001b[39m:]  \u001b[38;5;66;03m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\NLP2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\NLP2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\NLP2\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:597\u001b[39m, in \u001b[36mT5LayerSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions, cache_position)\u001b[39m\n\u001b[32m    585\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    586\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    587\u001b[39m     hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    594\u001b[39m     cache_position=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    595\u001b[39m ):\n\u001b[32m    596\u001b[39m     normed_hidden_states = \u001b[38;5;28mself\u001b[39m.layer_norm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m597\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.SelfAttention(\n\u001b[32m    598\u001b[39m         normed_hidden_states,\n\u001b[32m    599\u001b[39m         mask=attention_mask,\n\u001b[32m    600\u001b[39m         position_bias=position_bias,\n\u001b[32m    601\u001b[39m         layer_head_mask=layer_head_mask,\n\u001b[32m    602\u001b[39m         past_key_value=past_key_value,\n\u001b[32m    603\u001b[39m         use_cache=use_cache,\n\u001b[32m    604\u001b[39m         output_attentions=output_attentions,\n\u001b[32m    605\u001b[39m         cache_position=cache_position,\n\u001b[32m    606\u001b[39m     )\n\u001b[32m    607\u001b[39m     hidden_states = hidden_states + \u001b[38;5;28mself\u001b[39m.dropout(attention_output[\u001b[32m0\u001b[39m])\n\u001b[32m    608\u001b[39m     outputs = (hidden_states,) + attention_output[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\NLP2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\NLP2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\NLP2\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:553\u001b[39m, in \u001b[36mT5Attention.forward\u001b[39m\u001b[34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions, cache_position)\u001b[39m\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    551\u001b[39m     position_bias_masked = position_bias\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m scores += position_bias_masked\n\u001b[32m    555\u001b[39m \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, key_length)\u001b[39;00m\n\u001b[32m    556\u001b[39m attn_weights = nn.functional.softmax(scores.float(), dim=-\u001b[32m1\u001b[39m).type_as(scores)\n",
      "\u001b[31mRuntimeError\u001b[39m: output with shape [4, 12, 512, 512] doesn't match the broadcast shape [4, 4, 12, 512, 512]"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.9 ('NLP2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11c7becfb3ab913813bd9aa8c016a376309325bf0d122f29c83fa200e32f21b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
